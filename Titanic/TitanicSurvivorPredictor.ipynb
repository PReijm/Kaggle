{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python379jvsc74a57bd08e451edc9633588bb69a413542b83b6526d3d4cf2c31aa0035645349f27e347e",
   "display_name": "Python 3.7.9 64-bit (conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Titanic - Machine Learning from Disaster\n",
    "\n",
    "part 2 - model training\n",
    "\n",
    "https://www.ahmedbesbes.com/blog/kaggle-titanic-competition \n",
    "\n",
    "https://towardsdatascience.com/explaining-feature-importance-by-example-of-a-random-forest-d9166011959e\n",
    "\n",
    "https://towardsdatascience.com/kaggle-titanic-competition-model-building-tuning-in-python-12f4f74436b5"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "#load packages\n",
    "import time\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from datetime import datetime\n",
    "from scipy.stats import spearmanr\n",
    "from scipy.cluster import hierarchy\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "from sklearn.base import clone \n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.pipeline import Pipeline #need to specify names\n",
    "from sklearn.pipeline import make_pipeline #auto generates names\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "from sklearn.model_selection import GridSearchCV, KFold, cross_val_score, cross_val_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get data\n",
    "df = pd.read_csv('Data/clean.csv')\n",
    "\n",
    "df_train = df.dropna(subset=['Survived'])\n",
    "df_train = df_train.astype({'Survived': np.int})\n",
    "\n",
    "df_test = df[df.isnull().any(axis=1)]\n",
    "df_test = df_test.drop(['Survived'], axis=1)"
   ]
  },
  {
   "source": [
    "## Correlation study"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colormap = plt.cm.coolwarm\n",
    "plt.figure(figsize=(25,15))\n",
    "sns.heatmap(df_train.corr().round(1), cmap=colormap, annot=True, linewidths=0.1)"
   ]
  },
  {
   "source": [
    "### Univariate Analysis"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot ratio survivers\n",
    "df_train['Survived'].value_counts().plot.pie(autopct='%.1f%%', explode=[0, 0.05])"
   ]
  },
  {
   "source": [
    "### Bivariate Analysis"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#group by class and survived\n",
    "df_train.groupby(['Pclass', 'Survived'])['Survived'].count() #used to count non-NA/null across axis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(x='Pclass', y='Survived', data=df_train, kind='point')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#chance of surviving by Fare\n",
    "df_train[['CategoricalFare', 'Survived']].groupby('CategoricalFare').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[['CategoricalFare', 'Survived']].groupby('CategoricalFare')['Survived'].mean().plot(kind='bar', figsize=(15, 7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#chance of surviving by Age\n",
    "df_train[['CategoricalAge', 'Survived']].groupby('CategoricalAge').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[['CategoricalAge', 'Survived']].groupby('CategoricalAge')['Survived'].mean().plot(kind='bar', figsize=(15, 7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, age_plot = plt.subplots(figsize = (10,5))\n",
    "age_plot = sns.histplot(data=df_train, x=\"Age\", hue=\"Survived\", multiple=\"dodge\", shrink=.8, kde=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#relation between age and fare of passengers\n",
    "sns.jointplot(x='Age', y='Fare', data=df_train, kind='hex')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#relation between sex and fare of passengers\n",
    "sns.catplot(x='Sex', y='Fare', data=df_train, kind='boxen')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#relation between age and sex of passengers\n",
    "sns.catplot(x='Sex', y='Age', data=df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#survival rate per deck\n",
    "df_train[['Deck', 'Survived']].groupby('Deck')['Survived'].mean().plot(kind='bar', figsize=(15, 7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#chance of surviving by ticket frequency, identical tickets indicate people travel together\n",
    "df_train[['TicketFrequency', 'Survived']].groupby('TicketFrequency').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#survival rater per family size\n",
    "df_train[['FamilySizeBin', 'Survived']].groupby('FamilySizeBin')['Survived'].mean().plot(kind='bar', figsize=(15, 7))"
   ]
  },
  {
   "source": [
    "Multivariate Analysis"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#relation between age, sex and class\n",
    "sns.catplot(x='Sex', y='Age', data=df_train, kind='box', hue='Pclass')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(x='Pclass', y='Age', data=df_train, kind='violin', hue='Sex')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#relation between age, fare, sex and class\n",
    "sns.relplot(x='Age', y='Fare', data=df_train, row='Sex', col='Pclass')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make a seperate plot dataset\n",
    "df_plot = df_train.copy()\n",
    "\n",
    "df_plot[\"Survived\"] = df_plot[\"Survived\"].replace({0:\"No\", 1:\"Yes\"}) \n",
    "df_plot[\"Pclass\"] = df_plot[\"Pclass\"].replace({1:\"1st\", 2:\"2nd\", 3:\"3rd\"}) \n",
    "df_plot[\"Embarked\"] = df_plot[\"Embarked\"].replace({1:\"Cherbourg\", 2:\"Queenstown\", 0:\"Southampton\"}) \n",
    "df_plot[\"Person\"] = df_plot[\"Person\"].replace({0:\"Female\", 1:\"Male\", 2:\"Child\"}) \n",
    "\n",
    "df_plot['SibSp'] = df_plot['SibSp'].astype(str)\n",
    "df_plot['Parch'] = df_plot['Parch'].astype(str)\n",
    "\n",
    "#create a subplot\n",
    "fig, axs = plt.subplots(ncols=3, nrows=2, figsize=(15,7))\n",
    "\n",
    "sns.histplot(data=df_plot, x=\"Survived\", hue=\"Embarked\", multiple=\"dodge\", shrink=.8, palette='bright', ax=axs[0,0])\n",
    "sns.histplot(data=df_plot, x=\"Survived\", hue=\"Pclass\", multiple=\"dodge\", shrink=.8, palette='bright', ax=axs[0,1])\n",
    "sns.histplot(data=df_plot, x=\"Survived\", hue=\"Person\", multiple=\"dodge\", shrink=.8, palette='bright', ax=axs[0,2])\n",
    "sns.histplot(data=df_plot, x=\"SibSp\", hue=\"Survived\", multiple=\"dodge\", shrink=.8, palette='bright', ax=axs[1,0])\n",
    "sns.histplot(data=df_plot, x=\"Parch\", hue=\"Survived\", multiple=\"dodge\", shrink=.8, palette='bright', ax=axs[1,1])\n",
    "sns.histplot(data=df_plot, x=\"Fare\", hue=\"Survived\", multiple=\"dodge\", shrink=.8, bins=10, palette='bright', ax=axs[1,2])"
   ]
  },
  {
   "source": [
    "## Model training"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df_train[df_train.columns.difference(['Survived'], sort=False)].values\n",
    "y_train = df_train[['Survived']].values.ravel()"
   ]
  },
  {
   "source": [
    "### Test some different models"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_comparison(models, X_train, y_train):\n",
    "    model_stats = {}\n",
    "    \n",
    "    for mname, minst in models.items():\n",
    "        model_scores = []\n",
    "\n",
    "        #create pipeline with scaler\n",
    "        model_pipe = make_pipeline(StandardScaler(), minst)\n",
    "\n",
    "        #fit model and save scores\n",
    "        model_pipe.fit(X_train, y_train)\n",
    "        model_scores.append(model_pipe.score(X_train, y_train))\n",
    "\n",
    "        #implement a cross validation score\n",
    "        scores = cross_val_score(model_pipe, X_train, y_train, cv=10, scoring='accuracy')\n",
    "        model_scores.append(scores.mean())\n",
    "        model_scores.append(scores.std())\n",
    "\n",
    "        #implement cross validation predictions\n",
    "        y_train_pred = cross_val_predict(model_pipe, X_train, y_train, cv=10)\n",
    "\n",
    "        #calculate precision and recall\n",
    "        precision = precision_score(y_train, y_train_pred)\n",
    "        recall = recall_score(y_train, y_train_pred)\n",
    "        model_scores.append(precision)\n",
    "        model_scores.append(recall)\n",
    "\n",
    "        #calculate F1 score\n",
    "        f1 = 2*(precision*recall)/(precision+recall)\n",
    "        model_scores.append(f1)\n",
    "\n",
    "        #calculate ROC AUC score\n",
    "        roc_auc = cross_val_score(model_pipe, X_train, y_train, cv=10, scoring='roc_auc').mean()\n",
    "        model_scores.append(roc_auc)\n",
    "\n",
    "        #create dictionary\n",
    "        model_stats[mname] = model_scores\n",
    "\n",
    "    colnames = ['accuracy','accuracy_cv_score','accuracy_cv_stddev',\n",
    "                'precision_score','recall_score','f1_score', 'roc_auc__cv_score']\n",
    "\n",
    "    df_model_stats = pd.DataFrame.from_dict(model_stats, orient='index', columns=colnames)\n",
    "    df_model_stats_ranked = df_model_stats.sort_values(by='accuracy_cv_score', ascending=False)\n",
    "\n",
    "    return df_model_stats_ranked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "models = {'RandomForestClassifier': RandomForestClassifier(random_state=42),\n",
    "          'LogisticRegression': LogisticRegression(random_state=42),\n",
    "          'SupportVectorClassifier': SVC(random_state=42),\n",
    "          'DecisionTreeClassifier': DecisionTreeClassifier(random_state=42),\n",
    "          'KNeighborsClassifier': KNeighborsClassifier(),\n",
    "          'GaussianNaiveBayes': GaussianNB(),\n",
    "          'Perceptron': Perceptron(random_state=42),\n",
    "          'LinearSVC': LinearSVC(dual=False, random_state=42),\n",
    "          'StochasticGradientDescent': SGDClassifier(random_state=42),\n",
    "          'LinearDiscriminantAnalysis': LinearDiscriminantAnalysis()}\n",
    "\n",
    "df_model_comparison = model_comparison(models, X_train, y_train)\n",
    "df_model_comparison"
   ]
  },
  {
   "source": [
    "### Filter features"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###drop certain features###\n",
    "\n",
    "#basis with all features (rfc validation score: 0.83166)\n",
    "drop_features_round0 = []\n",
    "\n",
    "#based on high related features in dendogram (rfc validation score: 0.82719)\n",
    "drop_features_round1 = ['CategoricalFare', 'CategoricalAge', 'IsMarried', 'DeckGroup', 'DeckGroup_M', 'Embarked_S', 'FamilySizeBin', 'FamilySizeBin_1', 'CategoricalAge_(-0.08, 16.0]']\n",
    "\n",
    "#based on the worst feature determined by the accurate feature importance (rfc validation score: 0.82833)\n",
    "drop_features_round2 = ['FamilySizeBin_4']\n",
    "\n",
    "#added the worst feature after refiting the model with drop features of round 2 (rfc validation score: 0.82381)\n",
    "drop_features_round3 = ['FamilySizeBin_4', 'Person']\n",
    "\n",
    "#added the worst feature after refiting the model with drop features of round 3 (rfc validation score: 0.82268)\n",
    "drop_features_round4 = ['FamilySizeBin_4', 'Person', 'Person_child']\n",
    "\n",
    "#added a feature wich did not change model performace because there were no negative features (rfc validation score: 0.82268)\n",
    "drop_features_round5 = ['FamilySizeBin_4', 'Person', 'Person_child', 'CategoricalAge_(16.0, 32.0]']\n",
    "\n",
    "#added the 3 worst features of round 5 to the list because they had the same score (rfc validation score: 0.8283)\n",
    "drop_features_round6 = ['FamilySizeBin_4', 'Person', 'Person_child', 'CategoricalAge_(16.0, 32.0]', 'Pclass', 'Deck', 'Title_Mrs'] \n",
    "\n",
    "#added the last worst feature because after this round there are no negative or zero impact features (rfc validation score: 0.82605)\n",
    "drop_features_round7 = ['FamilySizeBin_4', 'Person', 'Person_child', 'CategoricalAge_(16.0, 32.0]', 'Pclass', 'Deck', 'Title_Mrs', 'CategoricalFare_1']\n",
    "\n",
    "###keep certain features###\n",
    "\n",
    "#best features based on previous feature selection (rfc validation score: 0.83841)\n",
    "keep_features_round0 = ['Person', 'Sex', 'Person_male', 'Person_female', 'Title_Mr', 'Pclass', 'Title', 'Age', 'Fare', 'Pclass_3',\n",
    "                        'FamilySize', 'Deck', 'TicketFrequency', 'Title_Mrs', 'FamilySizeBin', 'HasCabin', 'CategoricalFare', 'Title_Miss', 'IsMarried', 'DeckGroup']\n",
    "\n",
    "#best features based on feature importance of the rfc and svc ensemble model (ensemble validation score: 0.8317)\n",
    "keep_features_round1 = ['Person', 'Sex', 'Person_male', 'Title_Mr', 'Pclass', 'Title', 'Age', 'Fare', 'Pclass_3',\n",
    "                        'FamilySize', 'Deck', 'TicketFrequency', 'Title_Mrs', 'FamilySizeBin', 'HasCabin', 'CategoricalFare', 'Title_Miss', 'IsMarried', 'DeckGroup']\n",
    "\n",
    "#update of best features based on feature importance of the rfc and svc ensemble model (ensemble validation score: 0.81036)\n",
    "keep_features_round2 = ['Sex', 'Title_Mr', 'Pclass', 'Title', 'Age', 'Fare', 'Pclass_3', 'FamilySize', 'Deck', 'TicketFrequency', 'Title_Mrs', \n",
    "                        'FamilySizeBin', 'HasCabin', 'CategoricalFare', 'Title_Miss', 'IsMarried', 'DeckGroup']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_option = 'Keep'                  # choose to drop or keep features\n",
    "drop_features = drop_features_round0    # which list of features to drop\n",
    "keep_features = keep_features_round0    # which list of features to keep\n",
    "\n",
    "if filter_option == 'Drop':\n",
    "\n",
    "    drop_features_train = ['Survived'] + drop_features\n",
    "    X_train_filter = df_train.drop(drop_features_train, axis=1).values \n",
    "    \n",
    "    drop_features_test = drop_features\n",
    "    X_test_filter = df_test.drop(drop_features_test, axis=1).values\n",
    "\n",
    "    features_included = df_train.drop(drop_features_train, axis=1).columns \n",
    "\n",
    "elif filter_option == 'Keep':\n",
    "    \n",
    "    X_train_filter = df_train[keep_features].values \n",
    "    X_test_filter = df_test[keep_features].values\n",
    "\n",
    "    features_included = df_train[keep_features].columns"
   ]
  },
  {
   "source": [
    "### Optimize model hyperparameters"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train_gs = X_train_filter             # complete or filtered dataset\n",
    "run_gs = False                          # doing a complete grid search  \n",
    "model_selected = 'lr'                   # choise between rfc, svc, lr for doing a grid search\n",
    "model_needed = ['rfc', 'svc']           # for which models is fitted estimator needed             \n",
    "ensembling = True\n",
    "\n",
    "if run_gs:\n",
    "\n",
    "    kf = KFold(n_splits = 10, shuffle = True, random_state = 42)\n",
    "\n",
    "    if model_selected == 'rfc':\n",
    "\n",
    "        pipe = Pipeline([('scale', StandardScaler()), ('rfc', RandomForestClassifier(random_state=42))])\n",
    "\n",
    "        params = {\n",
    "        'rfc__n_estimators': [50, 100, 250, 300, 400, 500, 600, 750],\n",
    "        'rfc__max_depth': [5, 8, 15, 25, 30, 50, 100],\n",
    "        'rfc__min_samples_split': [2, 5, 10, 15, 100],\n",
    "        'rfc__min_samples_leaf': [1, 2, 5, 10],\n",
    "        'rfc__max_features': ['auto', 'sqrt', 'log2'],\n",
    "        'rfc__bootstrap': [True, False] \n",
    "        }\n",
    "\n",
    "    elif model_selected == 'svc':\n",
    "\n",
    "        pipe = Pipeline([('scale', StandardScaler()), ('svc', SVC(random_state=42))])\n",
    "\n",
    "        params = {\n",
    "        'svc__kernel': ['linear', 'rbf', 'sigmoid'],\n",
    "        'svc__gamma': [0.0001, 0.001, 0.01, 1, 10],\n",
    "        'svc__C': [0.1, 1, 10, 100]\n",
    "        }\n",
    "\n",
    "    elif model_selected == 'lr':\n",
    "\n",
    "        pipe = Pipeline([('scale', StandardScaler()), ('lr', LogisticRegression(random_state=42))])\n",
    "\n",
    "        params = {\n",
    "        'lr__penalty': ['l1', 'l2', 'elasticnet', 'none'],\n",
    "        'lr__solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'],\n",
    "        'lr__C': [0.001, 0.01, 0.1, 1, 10, 100, 1000],\n",
    "        'lr__max_iter': list(range(100,1000,100))\n",
    "        }\n",
    "\n",
    "    model = GridSearchCV(pipe, params, scoring = 'accuracy', cv = kf, verbose = 2, n_jobs = -1, refit=True)\n",
    "    \n",
    "    start = time.time()\n",
    "    model.fit(X_train_gs, y_train)\n",
    "    end = time.time()\n",
    "\n",
    "    print(\"\\n Results from Grid Search\")\n",
    "    print(\"\\n Duration of GridSearch in minutes: \", round((end - start)/60, 2))\n",
    "    print(\"\\n The best estimator across ALL searched params:\\n\",model.best_estimator_)\n",
    "    print(\"\\n The best score across ALL searched params:\\n\",model.best_score_.round(6))\n",
    "    print(\"\\n The best parameters across ALL searched params:\\n\",model.best_params_)\n",
    "\n",
    "else:\n",
    "\n",
    "    train_gs_scaler = StandardScaler()\n",
    "    X_train_gs_scaled = train_gs_scaler.fit_transform(X_train_gs)\n",
    "    \n",
    "    estimators=[]\n",
    "\n",
    "    for models in model_needed:\n",
    "\n",
    "        if models == 'rfc':\n",
    "            \n",
    "            best_params_rfc = {\n",
    "                'n_estimators': 500,\n",
    "                'max_depth': 15,\n",
    "                'min_samples_split': 5,\n",
    "                'min_samples_leaf': 2,\n",
    "                'max_features': 'auto',\n",
    "                'bootstrap': True\n",
    "            }\n",
    "            \n",
    "            best_model_rfc = RandomForestClassifier(random_state=42, **best_params_rfc)\n",
    "            best_model_rfc.fit(X_train_gs_scaled, y_train)\n",
    "            estimators.append((\"rfc\", best_model_rfc))\n",
    "\n",
    "            print(\"\\n The training score of the best rfc model:\\n\",best_model_rfc.score(X_train_gs_scaled, y_train).round(5))\n",
    "            print(\" The validation score of the best rfc model:\\n\",cross_val_score(best_model_rfc, X_train_gs_scaled, y_train, cv=10, scoring='accuracy').mean().round(5))\n",
    "        \n",
    "        elif models == 'svc':     \n",
    "\n",
    "            best_params_svc = {     \n",
    "                'kernel': 'rbf',\n",
    "                'gamma': 0.001, \n",
    "                'C': 100\n",
    "            }\n",
    "\n",
    "            best_model_svc = SVC(random_state=42, **best_params_svc)\n",
    "            best_model_svc.fit(X_train_gs_scaled, y_train)\n",
    "            estimators.append((\"svc\", best_model_svc))\n",
    "\n",
    "            print(\"\\n The training score of the best svc model:\\n\",best_model_svc.score(X_train_gs_scaled, y_train).round(5))\n",
    "            print(\" The validation score of the best svc model:\\n\",cross_val_score(best_model_svc, X_train_gs_scaled, y_train, cv=10, scoring='accuracy').mean().round(5))\n",
    "\n",
    "        elif models == 'lr':     \n",
    "\n",
    "            best_params_lr = {     \n",
    "                'penalty': 'l1',\n",
    "                'solver': 'saga',\n",
    "                'C': 0.1,\n",
    "                'max_iter': 200 #at 100 the coef_ did not converge\n",
    "            }\n",
    "\n",
    "            best_model_lr = LogisticRegression(random_state=42, **best_params_lr)\n",
    "            best_model_lr.fit(X_train_gs_scaled, y_train)\n",
    "            estimators.append((\"lr\", best_model_lr))\n",
    "\n",
    "            print(\"\\n The training score of the best lr model:\\n\",best_model_lr.score(X_train_gs_scaled, y_train).round(5))\n",
    "            print(\" The validation score of the best lr model:\\n\",cross_val_score(best_model_lr, X_train_gs_scaled, y_train, cv=10, scoring='accuracy').mean().round(5))\n",
    "\n",
    "    if ensembling:\n",
    "\n",
    "        ensemble_model = VotingClassifier(estimators)      \n",
    "        ensemble_model.fit(X_train_gs_scaled, y_train)\n",
    "            \n",
    "        print(\"\\n The training score of the ensemble model:\\n\",ensemble_model.score(X_train_gs_scaled, y_train).round(5))\n",
    "        print(\" The validation score of the ensemble model:\\n\",cross_val_score(ensemble_model, X_train_gs_scaled, y_train, cv=10, scoring='accuracy').mean().round(5))"
   ]
  },
  {
   "source": [
    "### Check feature importances"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = ensemble_model     # which model to check feature importance\n",
    "weights = False                 # show weights of model, only possible with certain models\n",
    "accuracy = True                 # most accuracte option but also more time consuming\n",
    "use_trees = False               # whether to use the permutation importances option with trees \n",
    "\n",
    "if accuracy:\n",
    "\n",
    "    #save the benchmark score, need to be similar to training score above\n",
    "    benchmark_score = best_model.score(X_train_gs_scaled, y_train)\n",
    "    print('The benchmark score is: ', benchmark_score.round(5))\n",
    "\n",
    "    importances = []\n",
    "\n",
    "    for idx, item in enumerate(features_included):\n",
    "        \n",
    "        #make a clone of the best model\n",
    "        model_clone = clone(best_model)\n",
    "        model_clone.random_state = 42\n",
    "\n",
    "        #drop every time one of the features\n",
    "        X_train_fi = np.delete(X_train_gs_scaled, idx, axis = 1)\n",
    "\n",
    "        #refit the model and calculate training score\n",
    "        model_clone.fit(X_train_fi, y_train)\n",
    "        drop_col_score = model_clone.score(X_train_fi, y_train)\n",
    "        importances.append(benchmark_score - drop_col_score)\n",
    "\n",
    "    #save the data in a dataframe\n",
    "    accur_import = pd.DataFrame(data={\n",
    "            'Features': features_included,\n",
    "            'Bench - Drop': importances\n",
    "        })\n",
    "        \n",
    "    accur_import = accur_import.sort_values(by='Bench - Drop', ascending=False)\n",
    "    accur_import = accur_import.set_index(accur_import.columns[0])\n",
    "\n",
    "    #plot the results in a bar graph\n",
    "    accur_import.plot(kind='barh', figsize=(8, 12), title = 'Difference between benchmark and dropped feature model')\n",
    "    \n",
    "else:\n",
    "    \n",
    "    if use_trees == True:\n",
    "        #plot feature importances (only with trees)\n",
    "        importances = best_model.feature_importances_\n",
    "        sorted_indices = np.argsort(importances)[::-1]\n",
    "\n",
    "        plt.figure(figsize=(15, 7))\n",
    "        plt.title('Feature Importances')\n",
    "        plt.bar(range(X_train_gs_scaled.shape[1]), importances[sorted_indices], align='center')\n",
    "        plt.xticks(range(X_train_gs_scaled.shape[1]), features_included[sorted_indices], rotation=90)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    else:\n",
    "\n",
    "        if weights:\n",
    "            #plot weights assigned to features, only possible in a few situations\n",
    "            weights = pd.DataFrame(data={\n",
    "                'Features': features_included,\n",
    "                'Weights': best_model.coef_[0]\n",
    "            })\n",
    "\n",
    "            weights = weights.sort_values(by='Weights', ascending=False)\n",
    "            weights = weights.set_index(weights.columns[0])\n",
    "\n",
    "            weights.plot(kind='bar', figsize=(15, 7), title = 'Feature Weights')\n",
    "\n",
    "        else:\n",
    "            #plot permutation feature importance\n",
    "            permimport = permutation_importance(best_model, X_train_gs_scaled, y_train, n_repeats=10, random_state=42)\n",
    "            permimport_sorted_idx = permimport.importances_mean.argsort()\n",
    "\n",
    "            fig, ax = plt.subplots(figsize=(8, 12))\n",
    "            ax.boxplot(permimport.importances[permimport_sorted_idx].T, vert=False, labels=features_included[permimport_sorted_idx])\n",
    "            ax.set_title(\"Permutation Importances\")\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check for correlated features\n",
    "fig, ax = plt.subplots(figsize=(15, 7))\n",
    "\n",
    "corr = spearmanr(X_train_gs_scaled).correlation\n",
    "corr_linkage = hierarchy.ward(corr)\n",
    "dendro = hierarchy.dendrogram(corr_linkage, labels=features_included, ax=ax, leaf_rotation=90)\n",
    "\n",
    "ax.set_title('Dendrogram')\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "source": [
    "## Make a submission"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#select the final model \n",
    "best_model = ensemble_model\n",
    "\n",
    "#select which data to use for making predictions\n",
    "X_test_pred = X_test_filter\n",
    "\n",
    "#use scaler of training dataset\n",
    "X_test_scaled = train_gs_scaler.transform(X_test_pred)\n",
    "\n",
    "#make the predictions\n",
    "pred_test = best_model.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check if the model is better as a basic 1 or 0 prediction\n",
    "display(pred_test.mean())\n",
    "display(pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = pd.DataFrame()\n",
    "sub['PassengerId'] = range(892, 1310, 1)\n",
    "sub['Survived'] = pred_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def savesub (df):\n",
    "     ts = datetime.now().strftime(\"%d%m%y_%H%M\")\n",
    "    \n",
    "     files = next(os.walk(\"C:/Users/Peter/Documents/Kaggle/Titanic/Submissions/\"))[2]\n",
    "     file_count = len(files)\n",
    "     \n",
    "     name_file = 'submission{}_{}.csv'.format(file_count, ts)\n",
    "     print(name_file)\n",
    "\n",
    "     name_model = 'model{}_{}.pkl'.format(file_count, ts)\n",
    "     print(name_model)\n",
    "     \n",
    "     #save submission\n",
    "     df.to_csv(r'C:/Users/Peter/Documents/Kaggle/Titanic/Submissions/' + name_file, index=False)\n",
    "\n",
    "     #save model\n",
    "     joblib.dump(best_model, r'C:/Users/Peter/Documents/Kaggle/Titanic/Models/' + name_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "savesub(sub)"
   ]
  }
 ]
}